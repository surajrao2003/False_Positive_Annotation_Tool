# False Positive Annotation Review Tool

## Overview
The **False Positive Annotation Review Tool** helps you quickly review and manage false positive predictions generated by machine learning models (e.g., Grounding DINO) in the **COCO dataset format**. You can accept, reject, or edit bounding box annotations using simple keyboard shortcuts, speeding up the process of improving a dataset without manual annotation.

## Use Case
In case a dataset is not properly annotated, you can run a model like **Grounding DINO** to get predictions. These predictions can be reviewed and refined using this tool and later merged with the original ground truth of the dataset. This process saves a lot of time compared to manually adding bounding boxes to the dataset.

## Workflow
1. **Generate False Positives**: Use model predictions to identify false positives by IOU thresholding with ground truth annotations using the **`false_positives_extractor.py`** script.
2. **Review False Positives**: Review and filter false positives using the **`annotation_review_script.py`**.
3. **Merge Accepted Annotations**: Merge the accepted false positives with your original ground truth dataset using **`merge_annotation_files.py`**.
4. As a result, you will have the ground truth + false positive annotations merged into your original dataset, ready for further use.

## Keyboard Shortcuts for the annotation tool
- `a`: Accept the current bounding box annotation
- `r`: Reject the current bounding box annotation
- `n`: Skip the current annotation without decision
- `p`: Go to the previous annotation
- `e`: Edit the annotation category (enter the new category ID)
- `l`: Log problematic images (for later review purposes, the image names will be added to a .txt file)
- `q`: Quit and save reviewed annotations

## Visual Indicators
- **Blue Box**: Unreviewed annotation
- **Green Box**: Accepted annotation
- **Red Box**: Rejected annotation

## Prerequisites
- Python 3.x
- OpenCV
- tkinter

## Usage

### Step 1: Prepare Your Dataset and Annotations
- Ensure your dataset annotations are in **COCO format** (.json file must be in COCO format).
- Run **`false_positives_extractor.py`** to generate false positives from model predictions and ground truth annotations. (Follow instructions given in the script.)

```bash
python false_positives_extractor.py
```
### Step 2: Run the False Positive Annotation Review Tool
Execute the **`annotation_review_script.py`**:

```bash
python annotation_review_script.py
```

### Step 3: Review Annotations
Review the generated false positives interactively:
- Use keyboard shortcuts to accept, reject, or edit annotations. You can even change the keyboard shortcuts according to your convenience within the script.
- The tool displays images with bounding boxes, and you can track your review decisions.

### Step 4: Save and Merge
Once finished, the accepted false positives will be saved in the output JSON file. Use **`merge_annotation_files.py`** to merge this file with your original ground truth annotations and update the dataset.

```bash
python merge_annotation_files.py
```

## Conclusion
Designed to save time by allowing quick review of model-generated false positives, minimizing manual annotation effort.
